{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Complete Lord of the Rings Text -- No Book or Chapter Divisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete texts of Lord of the Rings can be located at \n",
    "# 'https://archive.org/details/TheLordOfTheRing1TheFellowshipOfTheRing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./the_lord_of_the_rings/Lord_of_the_Rings_complete.txt', 'r') as file:\n",
    "    lotr = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2512368"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the size of the string to double check\n",
    "# 2_512_368 characters \n",
    "len(lotr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving the corpus as one long string is optimal for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Corpus for NLP (Tokenize, Punctuation Removal, Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem import WordNetLemmatizer;\n",
    "from nltk.tokenize import RegexpTokenizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and tokenize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # Separating by word using Regular Expressions \n",
    "lotr_tokens = tokenizer.tokenize(lotr)\n",
    "\n",
    "# Keeping capitalization because I want the model to treat the proper nouns accordingly. Names are important in LotR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hobbits'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_tokens[2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowing how sensitive LDA models can be regarding word counts; I'm reticent to remove any additional stopwords. \n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Some of these are lemma so I'll do a second stopword sweep during CountVectorization and Tf-Idf. \n",
    "# I'd rather add/subtract stopwords early in the process. \n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Length of original list: 482056 words\n",
      "\n",
      "Length of list after stopwords removal: 228086 words\n"
     ]
    }
   ],
   "source": [
    "# Removes stopwords and gives a count of words before and after removal\n",
    "\n",
    "lotr_clean = [word for word in lotr_tokens if word.lower() not in stopwords]\n",
    "print(\"=\"*90)\n",
    "print(f'Length of original list: {len(lotr_tokens)} words\\n')\n",
    "print(f'Length of list after stopwords removal: {len(lotr_clean)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'workshops'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_clean[2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lotr_tokens_lems = [lemmatizer.lemmatize(i) for i in lotr_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'workshop'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Workshops' has been reduced to its root, or lemma, 'workshop'.\n",
    "\n",
    "lotr_tokens_lems[2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn;\n",
    "from sklearn.feature_extraction.text import CountVectorizer;\n",
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graciously borrowed from 'https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial' \n",
    "# We have essentially inherited and subclassed the original Sklearn's CountVectorizer class \n",
    "# and overwritten the build_analyzer method by implementing the lemmatizer for each list in the raw text matrix.\n",
    "\n",
    "lemm = WordNetLemmatizer() \n",
    "class LemmaCountVectorizer(CountVectorizer): # the parent class \n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer() # the child class\n",
    "        # 'super' is short for 'superimpose'\n",
    "        # super() helps to specifically call the Parent class method \n",
    "        # which has been overridden in the child class, from the child class.\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'too', 'fifteen', 'fill', 'again', 'therein', 'whither', 'per', 'such', 'interest', 'whom', 'nothing', 'yet', 'side', 'become', 'in', 'beforehand', 'de', 'an', 'some', 'four', 'no', 'nor', 'though', 'off', 'towards', 'hereupon', 'whereupon', 'are', 'among', 'five', 'wherein', 'until', 'much', 'fire', 'herself', 'see', 'whatever', 'upon', 'besides', 'afterwards', 'only', 'them', 'also', 'becomes', 'fifty', 'our', 'across', 'below', 'him', 'due', 'namely', 'himself', 'seems', 'few', 'through', 'his', 'sincere', 'above', 'be', 'anyway', 'hers', 'thereafter', 'what', 'already', 'former', 'mill', 'when', 'everywhere', 'except', 'thin', 'although', 'somewhere', 'etc', 'of', 'yours', 'my', 'around', 'part', 'all', 'whereby', 'being', 'else', 'ten', 'wherever', 'always', 'between', 'each', 'any', 'top', 'full', 'or', 'we', 'take', 'least', 'its', 'over', 'back', 'then', 'beyond', 'twelve', 'whence', 'latter', 'neither', 'show', 'must', 'well', 'nowhere', 'as', 'thence', 'if', 'amongst', 'mine', 'nevertheless', 'more', 'otherwise', 'would', 'almost', 'indeed', 'others', 'these', 'from', 'hereby', 'put', 'yourself', 'sometime', 'thereupon', 'seeming', 'everyone', 'hence', 'moreover', 'were', 'hasnt', 'with', 'her', 'inc', 'along', 'forty', 'here', 'co', 'can', 'mostly', 'bottom', 'me', 'found', 'behind', 'do', 'ltd', 'serious', 'therefore', 'toward', 'anyhow', 'but', 'than', 'up', 'un', 'twenty', 'someone', 'couldnt', 'get', 'ie', 'latterly', 'three', 'who', 'other', 'next', 'something', 'might', 'sixty', 'at', 'their', 'during', 'cant', 'several', 'anyone', 'which', 'should', 'formerly', 'enough', 'it', 'that', 'there', 'done', 'keep', 'seem', 'itself', 'elsewhere', 'herein', 'this', 'throughout', 'thru', 'very', 'where', 'same', 'to', 'since', 'ours', 'bill', 'noone', 'amount', 'became', 'own', 'because', 'why', 'down', 'hundred', 'thereby', 'everything', 'meanwhile', 'into', 'is', 'empty', 'first', 'front', 'onto', 'so', 'whoever', 'another', 'whole', 'against', 'will', 'yourselves', 'anywhere', 'about', 'detail', 'before', 'eleven', 'one', 'out', 'was', 'amoungst', 'been', 'for', 'have', 'most', 'system', 'perhaps', 'becoming', 'not', 're', 'a', 'somehow', 'now', 'without', 'either', 'via', 'whereas', 'two', 'whether', 'ourselves', 'give', 'the', 'under', 'could', 'describe', 'never', 'nobody', 'thick', 'thus', 'eight', 'hereafter', 'i', 'often', 'us', 'move', 'he', 'those', 'you', 'find', 'has', 'whenever', 'whereafter', 'every', 'cry', 'sometimes', 'nine', 'they', 'she', 'by', 'alone', 'however', 'third', 'name', 'con', 'cannot', 'last', 'both', 'seemed', 'further', 'on', 'may', 'go', 'your', 'within', 'made', 'rather', 'six', 'had', 'none', 'once', 'less', 'call', 'many', 'after', 'please', 'anything', 'whose', 'while', 'myself', 'am', 'eg', 'beside', 'themselves', 'together', 'even', 'ever', 'still', 'how', 'and'})\n"
     ]
    }
   ],
   "source": [
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [frozenset(stop_words.ENGLISH_STOP_WORDS), set(stopwords)] # Combines both stopword lists into a master list \n",
    " \n",
    "master_stops = ([list(x) for x in sets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['too', 'fifteen', 'fill', 'again', 'therein', 'whither', 'per', 'such', 'interest', 'whom', 'nothing', 'yet', 'side', 'become', 'in', 'beforehand', 'de', 'an', 'some', 'four', 'no', 'nor', 'though', 'off', 'towards', 'hereupon', 'whereupon', 'are', 'among', 'five', 'wherein', 'until', 'much', 'fire', 'herself', 'see', 'whatever', 'upon', 'besides', 'afterwards', 'only', 'them', 'also', 'becomes', 'fifty', 'our', 'across', 'below', 'him', 'due', 'namely', 'himself', 'seems', 'few', 'through', 'his', 'sincere', 'above', 'be', 'anyway', 'hers', 'thereafter', 'what', 'already', 'former', 'mill', 'when', 'everywhere', 'except', 'thin', 'although', 'somewhere', 'etc', 'of', 'yours', 'my', 'around', 'part', 'all', 'whereby', 'being', 'else', 'ten', 'wherever', 'always', 'between', 'each', 'any', 'top', 'full', 'or', 'we', 'take', 'least', 'its', 'over', 'back', 'then', 'beyond', 'twelve', 'whence', 'latter', 'neither', 'show', 'must', 'well', 'nowhere', 'as', 'thence', 'if', 'amongst', 'mine', 'nevertheless', 'more', 'otherwise', 'would', 'almost', 'indeed', 'others', 'these', 'from', 'hereby', 'put', 'yourself', 'sometime', 'thereupon', 'seeming', 'everyone', 'hence', 'moreover', 'were', 'hasnt', 'with', 'her', 'inc', 'along', 'forty', 'here', 'co', 'can', 'mostly', 'bottom', 'me', 'found', 'behind', 'do', 'ltd', 'serious', 'therefore', 'toward', 'anyhow', 'but', 'than', 'up', 'un', 'twenty', 'someone', 'couldnt', 'get', 'ie', 'latterly', 'three', 'who', 'other', 'next', 'something', 'might', 'sixty', 'at', 'their', 'during', 'cant', 'several', 'anyone', 'which', 'should', 'formerly', 'enough', 'it', 'that', 'there', 'done', 'keep', 'seem', 'itself', 'elsewhere', 'herein', 'this', 'throughout', 'thru', 'very', 'where', 'same', 'to', 'since', 'ours', 'bill', 'noone', 'amount', 'became', 'own', 'because', 'why', 'down', 'hundred', 'thereby', 'everything', 'meanwhile', 'into', 'is', 'empty', 'first', 'front', 'onto', 'so', 'whoever', 'another', 'whole', 'against', 'will', 'yourselves', 'anywhere', 'about', 'detail', 'before', 'eleven', 'one', 'out', 'was', 'amoungst', 'been', 'for', 'have', 'most', 'system', 'perhaps', 'becoming', 'not', 're', 'a', 'somehow', 'now', 'without', 'either', 'via', 'whereas', 'two', 'whether', 'ourselves', 'give', 'the', 'under', 'could', 'describe', 'never', 'nobody', 'thick', 'thus', 'eight', 'hereafter', 'i', 'often', 'us', 'move', 'he', 'those', 'you', 'find', 'has', 'whenever', 'whereafter', 'every', 'cry', 'sometimes', 'nine', 'they', 'she', 'by', 'alone', 'however', 'third', 'name', 'con', 'cannot', 'last', 'both', 'seemed', 'further', 'on', 'may', 'go', 'your', 'within', 'made', 'rather', 'six', 'had', 'none', 'once', 'less', 'call', 'many', 'after', 'please', 'anything', 'whose', 'while', 'myself', 'am', 'eg', 'beside', 'themselves', 'together', 'even', 'ever', 'still', 'how', 'and'], ['too', 'again', 'such', 'whom', \"hadn't\", 'in', 'weren', \"you're\", 'an', 'some', 'no', 'nor', 'off', \"wouldn't\", 'are', 'until', 'won', 'herself', 'only', 'them', \"shan't\", 'our', 'him', 'below', 'himself', \"that'll\", 'few', 'couldn', \"mustn't\", 'hasn', 'through', 'his', 'what', 'above', 'be', 'when', 'hers', \"mightn't\", 'just', 'wouldn', 'of', 'yours', \"needn't\", 'my', \"wasn't\", 'all', 'being', 'between', 'shouldn', 'doing', 'each', 'any', 'we', 'or', 'didn', 'then', 'its', 'over', 'don', \"she's\", 'as', 'if', 'theirs', 'more', \"weren't\", 'these', 'from', 'isn', 'yourself', 'were', 'mightn', 'with', \"you'll\", 'her', 'here', 'can', 'me', 'do', 'but', 'up', 'than', 'll', 'haven', 'who', 'other', 'their', 'at', 'during', 'which', 'd', 'should', 'it', 'that', 'does', 'there', 'itself', 'this', 'doesn', 'needn', 'very', \"doesn't\", 'where', 'same', 'to', 'o', 'ain', \"don't\", 'ours', 'own', 'wasn', \"shouldn't\", 'because', 'why', 'down', \"haven't\", 't', 'into', 'is', 'so', 'against', 'yourselves', 'will', 'out', 'about', 'was', 'before', \"aren't\", \"won't\", 'been', 'have', 'for', 'most', \"you've\", 've', 'not', 're', \"you'd\", 'a', 'now', 'shan', 'y', \"couldn't\", 'ourselves', 'the', 'under', \"isn't\", 'i', 'mustn', 'he', 'those', 'you', 'has', \"hasn't\", 'did', 'aren', 'they', 'she', 'by', 'm', \"should've\", 'both', 'on', 'further', \"it's\", 'your', 's', 'had', 'once', 'hadn', 'after', 'while', 'myself', 'am', 'ma', 'themselves', \"didn't\", 'having', 'how', 'and']]\n"
     ]
    }
   ],
   "source": [
    "# Our unaltered master list of stop words; including duplicates\n",
    "print(master_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lambda function to combine or flatten our list of lists\n",
    "\n",
    "flatten = lambda master_stops: [item for sublist in master_stops for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lambda function\n",
    "\n",
    "flat_master_stops = flatten(master_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'too', 'therein', 'per', 'yet', 'de', 'beforehand', 'towards', 'though', 'whatever', 'also', 'becomes', 'our', 'seems', \"that'll\", 'few', 'through', 'what', 'already', 'former', \"wasn't\", 'whereby', 'being', 'between', 'shouldn', 'take', 'beyond', 'nowhere', \"she's\", 'as', 'would', 'from', 'hereby', 'put', 'sometime', 'everyone', 'mightn', 'with', 'forty', 'here', 'found', 'serious', 'twenty', 'ie', 'three', 'sixty', 'needn', 'this', 'herein', 'same', 'o', 'bill', 'amount', 'became', 'why', 'hundred', 'everything', 'front', 'another', 'one', 'before', 'was', 'amoungst', 'have', \"you've\", 'a', 'without', 'either', 'y', 'two', 'could', 'describe', 'never', 'nobody', 'us', \"isn't\", 'hereafter', 'mustn', 'move', 'you', 'nine', 'aren', 'she', 'alone', 'last', \"it's\", 'made', 'call', 'anything', 'how', 'fifteen', 'interest', 'side', \"you're\", 'some', 'four', 'hereupon', 'much', 'see', 'afterwards', 'across', \"mustn't\", 'sincere', 'be', 'hers', \"mightn't\", 'just', 'of', \"needn't\", 'my', 'around', 'ten', 'else', 'or', 'didn', 'then', 'back', 'whence', 'mine', 'more', 'indeed', 'isn', 'yourself', 'hasnt', \"you'll\", 'inc', 'along', 'can', 'behind', 'therefore', 'toward', 'anyhow', 'up', 'latterly', 'who', 'other', 'might', 'at', 'several', 'should', 'there', 'done', 'throughout', 'elsewhere', \"doesn't\", 'where', 'because', 'is', 'anywhere', 'detail', \"won't\", 'most', 've', 'perhaps', 'becoming', 're', 'via', 'ourselves', 'give', 'the', 'under', 'thick', 'eight', 'often', 'those', 'find', 'has', 'every', 'did', 'm', 'go', 'whereupon', 'while', 'ma', \"shan't\", 'whither', 'nothing', \"hadn't\", 'weren', 'nor', \"wouldn't\", 'are', 'among', 'wherein', 'until', 'won', 'herself', 'upon', 'only', 'fifty', 'namely', 'due', 'couldn', 'above', 'when', 'anyway', 'mill', 'thin', 'somewhere', 'wouldn', 'etc', 'part', 'wherever', 'always', 'doing', 'any', 'least', 'twelve', 'latter', 'neither', 'show', 'don', 'amongst', 'these', 'others', 'seeming', 'me', 'ltd', 'than', 'haven', 'couldnt', 'get', 'their', 'during', 'which', 'd', 'that', 'seem', 'keep', 'thru', 'to', 'since', 'ain', \"don't\", 'ours', 'own', \"shouldn't\", 'down', 't', 'meanwhile', 'into', 'empty', 'onto', 'so', 'out', \"aren't\", 'been', \"you'd\", 'now', 'shan', 'he', 'whenever', 'cry', \"hasn't\", 'by', 'name', 'cannot', 'may', 'within', 'rather', 'had', 'hadn', 'less', 'together', 'beside', 'themselves', \"didn't\", 'even', 'still', 'and', 'fill', 'again', 'such', 'whom', 'become', 'in', 'an', 'no', 'off', 'five', 'fire', 'besides', 'them', 'below', 'him', 'himself', 'hasn', 'his', 'thereafter', 'everywhere', 'except', 'although', 'yours', 'all', 'each', 'top', 'full', 'we', 'its', 'over', 'must', 'well', 'thence', 'if', 'nevertheless', 'theirs', 'otherwise', \"weren't\", 'almost', 'thereupon', 'were', 'hence', 'moreover', 'her', 'co', 'mostly', 'bottom', 'do', 'but', 'un', 'someone', 'll', 'next', 'something', 'cant', 'anyone', 'formerly', 'enough', 'it', 'does', 'itself', 'doesn', 'very', 'noone', 'wasn', \"haven't\", 'thereby', 'first', 'whole', 'whoever', 'against', 'will', 'yourselves', 'about', 'eleven', 'system', 'for', 'not', 'somehow', 'whereas', 'whether', \"couldn't\", 'thus', 'i', 'whereafter', 'sometimes', 'they', 'however', 'third', 'con', \"should've\", 'both', 'seemed', 'further', 'on', 'your', 's', 'six', 'none', 'once', 'please', 'many', 'whose', 'after', 'myself', 'am', 'eg', 'ever', 'having'}\n"
     ]
    }
   ],
   "source": [
    "# Unique stopwords in our master list \n",
    "\n",
    "print(set(flat_master_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, # Ignore terms that a document frequency higher that .95 of stopwords\n",
    "                                     min_df=2, # Ignore terms that have a document frequency of 2 \n",
    "                                     stop_words='english', # Second sweep -- words listed above\n",
    "                                     decode_error='ignore') # Ignore UnicodeDecodeError \n",
    "tf = tf_vectorizer.fit_transform(lotr_tokens_lems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuals on Lemmatized and Cleaned Corpus -- No Additional Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": [
           4111,
           1989,
           1321,
           1291,
           1253,
           1193,
           1137,
           1129,
           1121,
           949,
           840,
           817,
           817,
           800,
           790,
           788,
           772,
           759,
           759,
           756,
           722,
           722,
           708,
           685,
           682,
           664,
           663,
           642,
           632,
           619,
           598,
           596,
           586,
           584,
           578,
           564,
           561,
           538,
           530,
           527,
           520,
           511,
           505,
           500,
           488,
           486,
           485,
           485,
           483,
           472
          ],
          "colorscale": "Jet"
         },
         "text": "Word counts",
         "type": "bar",
         "uid": "9f670692-56ef-4aff-9b84-00002168bc00",
         "x": [
          "said",
          "frodo",
          "come",
          "sam",
          "came",
          "great",
          "long",
          "like",
          "gandalf",
          "away",
          "day",
          "far",
          "time",
          "way",
          "hobbit",
          "dark",
          "men",
          "hand",
          "know",
          "eye",
          "aragorn",
          "old",
          "went",
          "pippin",
          "shall",
          "light",
          "thing",
          "tree",
          "road",
          "looked",
          "merry",
          "say",
          "little",
          "thought",
          "land",
          "ring",
          "night",
          "saw",
          "ore",
          "good",
          "shadow",
          "stood",
          "end",
          "think",
          "heard",
          "stone",
          "black",
          "foot",
          "king",
          "passed"
         ],
         "y": [
          4111,
          1989,
          1321,
          1291,
          1253,
          1193,
          1137,
          1129,
          1121,
          949,
          840,
          817,
          817,
          800,
          790,
          788,
          772,
          759,
          759,
          756,
          722,
          722,
          708,
          685,
          682,
          664,
          663,
          642,
          632,
          619,
          598,
          596,
          586,
          584,
          578,
          564,
          561,
          538,
          530,
          527,
          520,
          511,
          505,
          500,
          488,
          486,
          485,
          485,
          483,
          472
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Top 50 Word Frequencies after Preprocessing with Unaltered Stop Word List"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"7e216fb8-58a2-4efb-ba6a-37d31475da75\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"7e216fb8-58a2-4efb-ba6a-37d31475da75\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '7e216fb8-58a2-4efb-ba6a-37d31475da75',\n",
       "                        [{\"marker\": {\"color\": [4111, 1989, 1321, 1291, 1253, 1193, 1137, 1129, 1121, 949, 840, 817, 817, 800, 790, 788, 772, 759, 759, 756, 722, 722, 708, 685, 682, 664, 663, 642, 632, 619, 598, 596, 586, 584, 578, 564, 561, 538, 530, 527, 520, 511, 505, 500, 488, 486, 485, 485, 483, 472], \"colorscale\": \"Jet\"}, \"text\": \"Word counts\", \"type\": \"bar\", \"uid\": \"939ac967-7f79-4515-8710-0c83682aa4e2\", \"x\": [\"said\", \"frodo\", \"come\", \"sam\", \"came\", \"great\", \"long\", \"like\", \"gandalf\", \"away\", \"day\", \"far\", \"time\", \"way\", \"hobbit\", \"dark\", \"men\", \"hand\", \"know\", \"eye\", \"aragorn\", \"old\", \"went\", \"pippin\", \"shall\", \"light\", \"thing\", \"tree\", \"road\", \"looked\", \"merry\", \"say\", \"little\", \"thought\", \"land\", \"ring\", \"night\", \"saw\", \"ore\", \"good\", \"shadow\", \"stood\", \"end\", \"think\", \"heard\", \"stone\", \"black\", \"foot\", \"king\", \"passed\"], \"y\": [4111, 1989, 1321, 1291, 1253, 1193, 1137, 1129, 1121, 949, 840, 817, 817, 800, 790, 788, 772, 759, 759, 756, 722, 722, 708, 685, 682, 664, 663, 642, 632, 619, 598, 596, 586, 584, 578, 564, 561, 538, 530, 527, 520, 511, 505, 500, 488, 486, 485, 485, 483, 472]}],\n",
       "                        {\"title\": {\"text\": \"Top 50 Word Frequencies after Preprocessing with Unaltered Stop Word List\"}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7e216fb8-58a2-4efb-ba6a-37d31475da75');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Signed up for plotly. See Jeff Hale's article: \n",
    "# https://towardsdatascience.com/its-2019-make-your-data-visualizations-interactive-with-plotly-b361e7d45dc6\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "count_vec = np.asarray(tf.sum(axis=0)).ravel()\n",
    "zipped = list(zip(feature_names, count_vec))\n",
    "x, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n",
    "\n",
    "# Now I want to extract out on the top 15 and bottom 15 words\n",
    "Y = np.concatenate([y[0:15], y[-16:-1]])\n",
    "X = np.concatenate([x[0:15], x[-16:-1]])\n",
    "\n",
    "# Plotting the Plot.ly plot for the Top 50 word frequencies\n",
    "data = [go.Bar(\n",
    "            x = x[0:50],\n",
    "            y = y[0:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = y[0:50]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 Word Frequencies after Preprocessing with Unaltered Stop Word List'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')\n",
    "\n",
    "# The words 'said', 'come', and 'came' are overrepresented. Going to make them stopwords and reanalyze. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The words 'said', 'come', and 'came' are overrepresented. Going to make them stopwords and reanalyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowing how sensitive LDA models can be regarding word counts; I'm reticent to remove any additional stopwords. \n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')  # changing this list can have a dramatic effect on topic modelling\n",
    "                                                    # because they use word counts (LDA more that NMF models)\n",
    "    \n",
    "stopwords.append('said') # removed due to disproportional frequency\n",
    "stopwords.append('come')\n",
    "stopwords.append('came')\n",
    "\n",
    "# Additional stopwords, like proper names, could dramatically alter results. Keeping them to preserve the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords using our new list\n",
    "lotr_clean = [word for word in lotr_tokens if word.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lotr_tokens_lems = [lemmatizer.lemmatize(i) for i in lotr_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(lotr_tokens_lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": [
           1989,
           1291,
           1193,
           1137,
           1129,
           1121,
           949,
           840,
           817,
           817,
           800,
           790,
           788,
           772,
           759,
           759,
           756,
           722,
           722,
           708,
           685,
           682,
           664,
           663,
           642,
           632,
           619,
           598,
           596,
           586,
           584,
           578,
           564,
           561,
           538,
           530,
           527,
           520,
           511,
           505,
           500,
           488,
           486,
           485,
           485,
           483,
           472,
           469,
           466,
           465
          ],
          "colorscale": "Jet"
         },
         "text": "Word counts",
         "type": "bar",
         "uid": "035f99ad-2658-4fe8-bd0d-44ebcb43b05f",
         "x": [
          "frodo",
          "sam",
          "great",
          "long",
          "like",
          "gandalf",
          "away",
          "day",
          "far",
          "time",
          "way",
          "hobbit",
          "dark",
          "men",
          "hand",
          "know",
          "eye",
          "aragorn",
          "old",
          "went",
          "pippin",
          "shall",
          "light",
          "thing",
          "tree",
          "road",
          "looked",
          "merry",
          "say",
          "little",
          "thought",
          "land",
          "ring",
          "night",
          "saw",
          "ore",
          "good",
          "shadow",
          "stood",
          "end",
          "think",
          "heard",
          "stone",
          "black",
          "foot",
          "king",
          "passed",
          "voice",
          "hill",
          "left"
         ],
         "y": [
          1989,
          1291,
          1193,
          1137,
          1129,
          1121,
          949,
          840,
          817,
          817,
          800,
          790,
          788,
          772,
          759,
          759,
          756,
          722,
          722,
          708,
          685,
          682,
          664,
          663,
          642,
          632,
          619,
          598,
          596,
          586,
          584,
          578,
          564,
          561,
          538,
          530,
          527,
          520,
          511,
          505,
          500,
          488,
          486,
          485,
          485,
          483,
          472,
          469,
          466,
          465
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Top 50 Word Frequencies after Preprocessing with Additional Stopwords"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"80e7d9fe-57f0-41a5-85d8-979b1da57703\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"80e7d9fe-57f0-41a5-85d8-979b1da57703\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '80e7d9fe-57f0-41a5-85d8-979b1da57703',\n",
       "                        [{\"marker\": {\"color\": [1989, 1291, 1193, 1137, 1129, 1121, 949, 840, 817, 817, 800, 790, 788, 772, 759, 759, 756, 722, 722, 708, 685, 682, 664, 663, 642, 632, 619, 598, 596, 586, 584, 578, 564, 561, 538, 530, 527, 520, 511, 505, 500, 488, 486, 485, 485, 483, 472, 469, 466, 465], \"colorscale\": \"Jet\"}, \"text\": \"Word counts\", \"type\": \"bar\", \"uid\": \"f17a3d06-9d54-4d79-8b8b-6755631d15ca\", \"x\": [\"frodo\", \"sam\", \"great\", \"long\", \"like\", \"gandalf\", \"away\", \"day\", \"far\", \"time\", \"way\", \"hobbit\", \"dark\", \"men\", \"hand\", \"know\", \"eye\", \"aragorn\", \"old\", \"went\", \"pippin\", \"shall\", \"light\", \"thing\", \"tree\", \"road\", \"looked\", \"merry\", \"say\", \"little\", \"thought\", \"land\", \"ring\", \"night\", \"saw\", \"ore\", \"good\", \"shadow\", \"stood\", \"end\", \"think\", \"heard\", \"stone\", \"black\", \"foot\", \"king\", \"passed\", \"voice\", \"hill\", \"left\"], \"y\": [1989, 1291, 1193, 1137, 1129, 1121, 949, 840, 817, 817, 800, 790, 788, 772, 759, 759, 756, 722, 722, 708, 685, 682, 664, 663, 642, 632, 619, 598, 596, 586, 584, 578, 564, 561, 538, 530, 527, 520, 511, 505, 500, 488, 486, 485, 485, 483, 472, 469, 466, 465]}],\n",
       "                        {\"title\": {\"text\": \"Top 50 Word Frequencies after Preprocessing with Additional Stopwords\"}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('80e7d9fe-57f0-41a5-85d8-979b1da57703');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "count_vec = np.asarray(tf.sum(axis=0)).ravel()\n",
    "zipped = list(zip(feature_names, count_vec))\n",
    "x, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n",
    "\n",
    "# Now I want to extract out on the top 15 and bottom 15 words\n",
    "Y = np.concatenate([y[0:15], y[-16:-1]])\n",
    "X = np.concatenate([x[0:15], x[-16:-1]])\n",
    "\n",
    "# Plotting the Plot.ly plot for the Top 50 word frequencies\n",
    "data = [go.Bar(\n",
    "            x = x[0:50],\n",
    "            y = y[0:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = y[0:50]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 Word Frequencies after Preprocessing with Additional Stopwords'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'm comfortable with Frodo and Sam being at the top of our word frquencies since they're the main characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
