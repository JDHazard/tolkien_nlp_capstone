{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re, string\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Complete Lord of the Rings Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete texts of Lord of the Rings can be located at \n",
    "# 'https://archive.org/details/TheLordOfTheRing1TheFellowshipOfTheRing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./the_lord_of_the_rings_text/Lord_of_the_Rings_complete.txt', 'r') as file:\n",
    "    lotr = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2512368"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the size of the string to double check\n",
    "len(lotr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving the corpus as one long string is optimal for NLP, LDA, and NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Text for NLP (Tokenize, Punctuation Removal, Stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "\n",
    "\n",
    "import sklearn;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
    "\n",
    "# Please see my 'preprocessing_stopwords_plotly.ipynb' notebook for complete annotated preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and tokenize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lotr_tokens = tokenizer.tokenize(lotr)\n",
    "\n",
    "# Keeping capitalization because I want the model to treat the proper nouns accordingly. Names are important in LotR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hobbits'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_tokens[2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on previous models, it's imperative to add some stop words. \n",
    "# add stopwords - changing this list can have a dramatic effect on results in the LDA model because it uses word counts\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('said') # removed due to disproportional frequency\n",
    "stopwords.append('come')\n",
    "stopwords.append('came')\n",
    "\n",
    "# Additional stopwords, like proper names, could dramatically alter results. Keeping them to preserve the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Length of original list: 482056 words\n",
      "\n",
      "Length of list after stopwords removal: 221549 words\n"
     ]
    }
   ],
   "source": [
    "# Removes stopwords \n",
    "lotr_clean = [word for word in lotr_tokens if word.lower() not in stopwords]\n",
    "print(\"=\"*90)\n",
    "print(f'Length of original list: {len(lotr_tokens)} words\\n')\n",
    "print(f'Length of list after stopwords removal: {len(lotr_clean)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flowers'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_clean[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lotr_tokens_lems = [lemmatizer.lemmatize(i) for i in lotr_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flower'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_tokens_lems[1891]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Lemmmatizing and CountVectorizer into one Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graciously borrowed from 'https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial' \n",
    "# We have essentially inherited and subclassed the original Sklearn's CountVectorizer class \n",
    "# and overwritten the build_analyzer method by implementing the lemmatizer for each list in the raw text matrix.\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(lotr_tokens_lems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Our LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF;\n",
    "from sklearn.preprocessing import normalize;\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Using the SKlearn LDA and NMF models -- The Gensim topic models produced uninterpreable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=21, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558095001.606483\n"
     ]
    }
   ],
   "source": [
    "lda.fit(tf)\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graciously borrowed from 'https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial'\n",
    "# Define helper function to print top words\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_): # enumerate keeps count of iterations \n",
    "        message = \"\\nTopic #{}:\".format(index) \n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        # list comprehension inside .join function\n",
    "        print(message)\n",
    "        print(\"=\"*70) # prints 70 '=' signs as separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model: \n",
      "\n",
      "Topic #0:heard left took door enemy near fair darkness green began people son close short poor young bright unless room beast\n",
      "======================================================================\n",
      "\n",
      "Topic #1:hand road face head word look saruman evil lie tall star rock line evening elrond shape stream led standing bowed\n",
      "======================================================================\n",
      "\n",
      "Topic #2:merry city rider legolas rohan wish save boromir guess followed caught used running deed ship try strider tongue shagrat nearly\n",
      "======================================================================\n",
      "\n",
      "Topic #3:way little white soon fall mina help half taken run peril rising bank large forest escape huge afraid tunnel sighed\n",
      "======================================================================\n",
      "\n",
      "Topic #4:time looked tower lay thjoden hour pa grew sea answer bilbo valley come song stair hidden quickly big westward pain\n",
      "======================================================================\n",
      "\n",
      "Topic #5:high seen tell round felt better south captain looking silent earth trouble treebeard halted weary low bag bent father swift\n",
      "======================================================================\n",
      "\n",
      "Topic #6:far aragorn saw stone let suddenly east moment air river strength le asked cut cast kept worse tidings passing free\n",
      "======================================================================\n",
      "\n",
      "Topic #7:land night good hope set knew ground want walked told beregond beneath care living moon bridge bitter straight chapter fool\n",
      "======================================================================\n",
      "\n",
      "Topic #8:eye think heart grey dead year company ran arm new morning guard wild glad strong slope golden peace edge galadriel\n",
      "======================================================================\n",
      "\n",
      "Topic #9:sam long wall cried house place make coming leave clear cold sky lost return broken middle best body nice chance\n",
      "======================================================================\n",
      "\n",
      "Topic #10:day hobbit faramir gimli sun rode jomer sat denethor wonder news country ready bear turning meet broke citadel age shadowfax\n",
      "======================================================================\n",
      "\n",
      "Topic #11:foot mr fear gone battle maybe wood gave longer sight returned stay memory bore hall riding creature command remained dear\n",
      "======================================================================\n",
      "\n",
      "Topic #12:know horse elf rose red brought lady silver ago food mark sauron doom cotton bring wait cliff realm speed hardly\n",
      "======================================================================\n",
      "\n",
      "Topic #13:shall king tree mountain yes shire spoke rest horn smjagol step forth point burden secret vast quick muttered blade floor\n",
      "======================================================================\n",
      "\n",
      "Topic #14:away black passed gollum fell slowly field power hold sent bit grass tirith feel hurt mouth aside falling desire finger\n",
      "======================================================================\n",
      "\n",
      "Topic #15:like went old stood gate hill man wide strange pale heavy home filled use stand dwarf love opened counsel understand\n",
      "======================================================================\n",
      "\n",
      "Topic #16:men ring sword west speak world journey leaf cloak work fallen hard mean silence dream spring narrow gloom ill steward\n",
      "======================================================================\n",
      "\n",
      "Topic #17:dark master going water gondor folk got mind sleep sound host laid window eastward isengard sign mist known companion foe\n",
      "======================================================================\n",
      "\n",
      "Topic #18:pippin lord ore shadow deep north mordor answered small called forward cloud course open farewell lifted ahead till reached hole\n",
      "======================================================================\n",
      "\n",
      "Topic #19:great end turned wind path friend ride drew tale mile plain precious laughed life sprang remember swiftly climbed dim flame\n",
      "======================================================================\n",
      "\n",
      "Topic #20:frodo gandalf thing say light thought voice right need hear war held death turn doubt chief ere smoke forgotten lying\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 20\n",
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Our NMF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization comparison to LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to get total word counts in documents\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform words with TfidfTransformer - This takes into account term frequency across and within documents\n",
    "\n",
    "word_counts = vectorizer.fit_transform(lotr_tokens_lems)\n",
    "\n",
    "tfidf_transform = TfidfTransformer(smooth_idf = False)\n",
    "\n",
    "words_tfidf = tfidf_transform.fit_transform(word_counts)\n",
    "\n",
    "# final_words = normalize(words_tfidf, norm = 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate NMF model and fit to tfidf transformed documents\n",
    "\n",
    "model = NMF(n_components = 50, init = 'nndsvd')\n",
    "\n",
    "# Set W as the document by topic matrix\n",
    "# Set H as the topic by word matrix\n",
    "\n",
    "W = model.fit_transform(words_tfidf)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign topic indices back to feature names - takes model, feature names from vectorizer, \n",
    "# and n_top_words as arguments. n_top_words selects the number of keywords per topic\n",
    "# renders differently than our previous function essentially does the same thing \n",
    "# Thanks to Nick Gayliard for inspiration \n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    lst = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        lst.append(message)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List numbered topic and top 10 words in the topic \n",
    "\n",
    "topics_nmf = (print_top_words(model, vectorizer.get_feature_names(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic #0: frodo behind end heard side passed though hill water first',\n",
       " 'Topic #1: sam behind end think passed side left much first master',\n",
       " 'Topic #2: great think end heard black side stone king though water',\n",
       " 'Topic #3: could stone passed though left voice hill let first suddenly',\n",
       " 'Topic #4: long think heard foot side passed water left hill lord',\n",
       " 'Topic #5: would end behind black heard side water let made much',\n",
       " 'Topic #6: like behind end think stood ever stone passed hill water',\n",
       " 'Topic #7: gandalf foot black side left heard king much made white',\n",
       " 'Topic #8: go behind end passed though black side left hill much',\n",
       " 'Topic #9: one behind black passed left though much first white two',\n",
       " 'Topic #10: back end ever heard black foot let water made lord',\n",
       " 'Topic #11: away behind end think passed though side hill water first',\n",
       " 'Topic #12: still heard foot passed though left white two word deep',\n",
       " 'Topic #13: see behind stood end think ever word two suddenly deep',\n",
       " 'Topic #14: many behind end think heard foot side water made first',\n",
       " 'Topic #15: upon end black heard good foot though much suddenly word',\n",
       " 'Topic #16: far end heard ever black saw passed left let first',\n",
       " 'Topic #17: time end black side though king water much lord gollum',\n",
       " 'Topic #18: day behind end think heard passed much suddenly first word',\n",
       " 'Topic #19: way behind end think foot side though water much master',\n",
       " 'Topic #20: last heard foot passed left much first suddenly master two',\n",
       " 'Topic #21: dark foot voice let first master white suddenly face hope',\n",
       " 'Topic #22: men black heard stone voice much king gollum white master',\n",
       " 'Topic #23: yet heard side foot passed though water voice let made',\n",
       " 'Topic #24: hand stood end passed made suddenly gollum wall word two',\n",
       " 'Topic #25: know black foot though side much king white hope seen',\n",
       " 'Topic #26: eye stood behind think stone though hill much lord first',\n",
       " 'Topic #27: well heard stone left king let water lord gollum master',\n",
       " 'Topic #28: old think foot heard left made water let going gollum',\n",
       " 'Topic #29: aragorn end side heard though water king face deep hope',\n",
       " 'Topic #30: went stood think stone hill king lord master fell deep',\n",
       " 'Topic #31: hobbit stood ever foot though king made two word head',\n",
       " 'Topic #32: must behind think passed left much first white wall word',\n",
       " 'Topic #33: seemed end heard though water master turned grey fear door',\n",
       " 'Topic #34: pippin think side stone left passed water hill king lord',\n",
       " 'Topic #35: shall behind stood end much first suddenly fell set felt',\n",
       " 'Topic #36: even black left though let gimli never indeed faramir began',\n",
       " 'Topic #37: may heard black let much gollum master first fell turned',\n",
       " 'Topic #38: light think heard black side let voice much first master',\n",
       " 'Topic #39: thing end think side water hill let first master suddenly',\n",
       " 'Topic #40: tree stood think though hill king lord master two white',\n",
       " 'Topic #41: road heard black foot stone water going suddenly found fell',\n",
       " 'Topic #42: looked end ever voice though water let suddenly white face',\n",
       " 'Topic #43: merry behind stood end think side gollum face found fell',\n",
       " 'Topic #44: say heard black foot left though water lord suddenly deep',\n",
       " 'Topic #45: thought behind stood think much first word wall head deep',\n",
       " 'Topic #46: little end ever heard stone water though lord master fell',\n",
       " 'Topic #47: land ever think heard saw stone passed hill first lord',\n",
       " 'Topic #48: ring good think black foot side passed hill much word',\n",
       " 'Topic #49: night saw good behind shadow stood voice left king made']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nmf topic assignment\n",
    "\n",
    "topics_nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions: \n",
    "### Topic Modelling for works of fiction requires further research. My hypothesis is that given the assumptions made by both the LDA and NMF models easily interpretable results are both difficult to reach and hard to interpret. Close examination of these assumptions, word counts following a specific distribution, for example,  should be made before continuing with this line of questioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
